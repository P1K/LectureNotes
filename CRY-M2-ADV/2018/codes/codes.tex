\documentclass[11pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage[hmargin=3cm,vmargin=2.4cm]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{fancyhdr}

\hypersetup{
  colorlinks=true,
  citecolor=NavyBlue,
  linkcolor=BrickRed,
  urlcolor=Violet
}

\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\href{https://www-ljk.imag.fr/membres/Pierre.Karpman/cry\_adv2018\_codes.pdf}{https://www-ljk.imag.fr/membres/Pierre.Karpman/cry\_adv2018\_codes.pdf}}

\theoremstyle{definition}
\newtheorem{defi}{Definition}
\def\defiautorefname{Definition}
\newtheorem{example}{Example}
\def\defiautorefname{Example}

\renewcommand{\labelitemi}{---}

\DeclareMathOperator\code{\mathcal{C}}
\DeclareMathOperator\bigo{\mathit{O}}
\DeclareMathOperator\hd{\mathit{hd}}
\DeclareMathOperator\wt{\mathit{wt}}
\DeclareMathOperator\RM{\mathrm{RM}}
\DeclareMathOperator\fun{\mathit{F}}
\DeclareMathOperator\eval{eval}
\DeclareMathOperator\Ber{Ber}
\DeclareMathOperator\rank{rank}
\DeclareMathOperator\vspan{span}
\DeclareMathOperator\gl{GL}
\DeclareMathOperator\walsh{\mathcal{W}}
\DeclareMathOperator\proj{\pi}
\newcommand\ftwo{\mathbb{F}_{2}}
\newcommand\zo{\{0,1\}}
\newcommand\randraw{\xleftarrow{{\scriptscriptstyle\$}}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\title{Advanced cryptology (GBX9SY06)\\
$\ast$\\
Some coding theory aspects (useful) in cryptography}
\date{2018-11}
\author{Pierre Karpman}

\begin{document}

\maketitle{}

\section{First definitions; examples}

A \emph{linear code} of length $n$ and dimension $k$ over a field $\mathbb{K}$ is a $k$-dimensional subspace of $\mathbb{K}^n$. In these notes, we will focus on \emph{binary} codes, for which $\mathbb{K} = \mathbb{F}_2$, or possibly an extension thereof.\footnote{Consequently, we may take the liberty of
equating subtraction with addition in any formula or algorithm. In order to minimize the confusion, we will try to make this systematic. Nonetheless, most of the discussion seamlessly generalises to other (finite) fields.}
An important characteristic of a code is the \emph{minimum distance} (for the Hamming distance $\hd(\cdot,\cdot)$) $d$ between two (distinct) codewords. Define $\wt(\bm{x})$, $\bm{x} \in \ftwo^n$ as the number of
non-zero coordinates of $\bm{x}$; $\hd(\bm{x},\bm{y})$ as $\wt(\bm{x}+\bm{y})$. Then the minimum distance of a code $\code$ is $\min_{\bm{x}\in\code,\bm{y}\neq\bm{c}\in\code} \hd(\bm{x},\bm{y})$, which by linearity of $\code$ is equivalent
to $\min_{\bm{x}\neq\bm{0}\in\code} \wt(\bm{x})$. The main parameters of length, dimension, and minimum distance of a code $\code$ are summarized by saying that $\code$ is an $[n,k,d]_{\ftwo}$ code. While determining $n$ and $k$ is usually straightforward,
it is in general a hard problem to compute $d$.

\smallskip

Given a code $\code$, it will often be necessary to have an explicit \emph{encoding} map $\bm{x} \in \ftwo^k \mapsto \bm{x}' \in \code$ from \emph{messages} to \emph{codewords}. Such a map can be easily obtained by sampling $k$ linearly-independent codewords
$\bm{g}_{1},\ldots,\bm{g}_{k}$ and forming the \emph{generator matrix} $\bm{G}$ whose rows are the $\bm{g}_i$s.\footnote{Note that the we use the convention that vectors are \emph{row} vectors, if not specified otherwise.} The encoding map is then
simply $\bm{x} \mapsto \bm{x}\times\bm{G}$. One should remark that in general the matrix $\bm{G}$ (and thence the encoding map) will not be unique, as it depends on the selected codewords. A specific class of generator matrices are the ones in
\emph{systematic form}, corresponding to block matrices $\begin{pmatrix}\bm{I}_k & \bm{A}\end{pmatrix}$, where $\bm{I}_k$ is the $k$-dimensional identity matrix and $\bm{A}$ is a \emph{redundancy} block. A code always admits at least one systematic encoder, up to a permutation of the coordinates
of its codewords. One may be obtained by selecting $k$ linearly-independent columns of a generator matrix $\bm{G}$; applying a column permutation on $\bm{G}$ such that those columns are in the first $k$ positions; and computing the reduced row-echelon form
of $\bm{G}$. In other words, one obtains an encoder in systematic form by finding a permutation matrix $\bm{P}$ such that $\bm{GP}$ is of the form $\bm{G}' := \begin{pmatrix}\bm{G}_1 & \bm{G}_2\end{pmatrix}$ where $\bm{G}_1$ is invertible, and by computing
$\bm{G}_1^{-1}\bm{G}'$.

From the existence of a systematic encoder, one deduces that the largest possible minimum distance (or weight) of an $[n,k]$ linear code is $d_{\text{MDS}} = n-k+1$, which is a special case of the Singleton bound. Indeed, the maximum possible
weight of any row of a systematic encoder is 1 on the left (identity) block, and $n-k$ on the right (redundancy) block. A code reaching this bound is called \emph{maximum-distance separable}, or MDS.

Finally, note that for some codes, there may exist alternative encoders
that do not explicitly use a generator matrix.

\begin{example}[AES MixColumn]
	Let $\bm{M}$ be the matrix used in the MixColumn operation of the AES block cipher. The code generated by $\begin{pmatrix}\bm{I}_4 & \bm{M}\end{pmatrix}$ is an MDS code of paramateres $[8,4,5]_{\mathbb{F}_{2^8}}$.
\end{example}

\begin{example}[Binary Reed-Muller codes]
	The binary Reed-Muller code of order $r$ and in $m$ variables $\RM(r,m)$ is the vector-space formed by the multi-point evaluations of $m$-variate Boolean functions of degree $\leq r$ over $\ftwo^m$. In other words,
	a message is a Boolean function, and its associated (Reed-Muller) codeword is obtained by evaluating it over its entire domain.
	
	The codewords of this code have length $2^m$ and form a space of dimension $k := \sum_{i=0}^r\binom{m}{i}$.
	It can be shown that the minimum weight of any codeword is $2^{m-r}$~\cite[Ch. 13, Thm. 3]{MS}. The parameters of $\RM(r,m)$ are thus $[2^m, k, 2^{m-r}]_{\ftwo}$.

	A (non-systematic) encoding of a message can be efficiently computed by using a fast Möbius transform. Due to its involutive nature, the same transform can also be used to decode a codeword back to a message. However, this does not correct any
	error and on its own it is thus of rather limited use.

	Reed-Muller codes also follow a recursive ``$(u,u+v)$'' decomposition. One has that $\RM(r+1,m+1) = \{u||(u+v), u \in \RM(r+1,m), v \in \RM(r,m)\}$. This follows from the fact that an $(m+1)$-variate Boolean function $\fun(X_1,\ldots,X_{m+1})$
	of degree at most $r+1$ can be written as $\fun^{0}(X_1,\ldots,X_m) + X_{m+1}\fun^{1}(X_1,\ldots,X_m)$, with
	%where $\fun^{0}$ (resp. $\fun^1$) is obtained by partial evaluation of $\fun$ by taking $X_{m+1} = 0$ (resp. $X_{m+1} = 1$).
	$\deg(\fun^0) \leq r+1$ and $\deg(\fun^1) \leq r$. Furthermore, if we write $\fun^{0+}$ the $m+1$-variate function whose monomials are identical
	to $\fun^0$, $\fun^{1+}$ for % the $m+1$-variate function equal to
	$X^{m+1}\fun^{1}$,
	and $\vec{X}_{1,m}$ a given assignement for the indeterminates $X_1,\ldots,X_m$,
	then
	we always have:
	\begin{itemize}
		\item $\eval(\fun^0,(\vec{X}_{1,m})) = \eval(\fun^{0+},(\vec{X}_{1,m},0)) = \eval(\fun^{0+},(\vec{X}_{1,m},1))$;
		\item $\eval(\fun^1,(\vec{X}_{1,m})) = \eval(\fun^{1+},(\vec{X}_{1,m},1))$;
		\item $\eval(\fun^{1+},(\vec{X}_{1,m},0)) = 0$;
	\end{itemize}
	and the decomposition follows. Finally, one may notice that this is essentially the same induction as the one used in the fast
	Möbius transform algorithm.
\end{example}

\medskip

Given a code $\code$, it is often important to be able to determine if a vector of its ambient space is a codeword or not.
This may be done using a map $\bm{x} \mapsto \bm{y}$ s.t. $\bm{y}$ is ``zero'' iff. $\bm{x} \in \code$. One typically implements this with a \emph{parity-check matrix} $\bm{H} \in \ftwo^{n-k\times n}$
which is a basis of the (right) kernel of a generator matrix $\bm{G}$ of $\code$; the corresponding map, of codomain $\ftwo^{n-k}$, is then $\bm{x} \mapsto \bm{H}\times\bm{x}$. Equivalently, $\bm{H}$ is made of $(n-k)$ linearly-independent vectors of $\ftwo^n$ whose scalar product with any element of $\code$ is zero, and $\bm{H}\bm{G}^t$ and
$\bm{G}\bm{H}^t$ are both zero matrices. A parity-check matrix generates the \emph{dual} of $\code$, written $\code^\bot$, which is thence an $[n,n-k]$ code. A code that is its own dual is called \emph{self-dual}.

\section{Information set decoding}

In this section we focus on the problem of finding ``low-weight'' codewords of  a code, which is also essentially equivalent to finding a close-by codeword to a given vector, i.e. to decode.
This is generally a hard problem for codes that do not exhibit any particular structure (for instance if they are defined from a uniformly random generator matrix), as it is NP-hard~\cite{codeshard}, but efficient algorithms may exist for some specific codes. For now we will focus on
``inefficient'' generic algorithms that work for any code, but we will later present a good \emph{list decoder} for (punctured and shortened) first-order Reed-Muller codes.

\smallskip

Let $\code$ be an $[n,k,d]$ code for which $\bm{G}$ is a generator matrix. Enumerating all the codewords of $\code$ can trivially be done in time $2^k$ by multiplying $\bm{G}$ by all the vectors of $\ftwo^k$. This immediately allows to find
a weight-$d$ codeword of $\code$ or to decode to the (or one of the) closest codeword(s),
but the cost is quickly prohibitive.

A first remark on the way to find better alternatives is that the problem that one needs to solve usually does not require to enumerate all the codewords of $\code$. For instance, one may not need to find a codeword of \emph{minimum} weight,
but finding one of weight less than a known bound may be enough. Similarly, in a decoding context, one may know an upper-bound on the error weight. It is then possible to use a probabilistic algorithm that stops when a ``good-enough''
solution has been found.

A second remark is that the decoding problem can be solved by finding low-weight codewords; this will be used to justify the fact that we solely focus on algorithms for the latter. This can be explained in the following way: let
$\bm{c} \in \code$ be an initial codeword, and $\bm{\hat{c}} = \bm{c} + \bm{e}$ be a noisy codeword obtained by adding a noise $\bm{e}$ of weight $w \leq  \textit{max weight} <
\lfloor(d-1)/2\rfloor$. Then $\bm{c}$
is the unique closest codeword to $\bm{\hat{c}}$, and $\bm{e}$ is the unique vector of weight $w$ in the affine subspace $\bm{e} + \code$. Furthermore, this latter vector can be found by searching for a ``codeword'' of weight
$w$ in the code generated by $\begin{pmatrix}\bm{G}\\ \bm{\hat{c}}\\ \end{pmatrix}$. This codeword will even be unique, as for any $\bm{c}' \neq \bm{c} \in \code$, $\wt(\bm{c}' + \bm{\hat{c}}) = \wt(\bm{c}' + \bm{c} + \bm{e})
	\geq \wt(\bm{c}' + \bm{c}) - \wt(e) \geq d - \textit{max weight} > w$.

\medskip

The first probabilistic alternative to exhaustive search that we present is quite simple~\cite{Prange,McEliece}. Given $\bm{G}$, randomly select $k$ linearly-independent columns; this is called an \emph{information set}. Then permute these columns
to the first $k$ positions of $\bm{G}$, and compute the reduced row-echelon form (i.e. compute an alternative generator matrix $\bm{G}'$ in systematic form, associated to the selected information set).
Finally, check if any of the resulting $k$ rows have a weight less than the input bound. The idea behind this algorithm is that any row of the obtained systematic encoder has by definition a very low weight of exactly one on its first $k$ positions,
and the weight on the remaining $n-k$ positions depends on a random codeword linear combination. One then hopes that for \emph{some} information sets, the weight on these latter positions we also be small, resulting in an overall low-weight codeword.
In other words, the algorithm will return a weight-$w$ codeword after examining a given information set if it is s.t. there is a codeword of weight 1 over the information set and of weight
$w - 1$ over its complement, the \emph{redundancy set}.

There is also a simple interpretation of this algorithm if one directly thinks of it in a ``decoding'' sense. An information set is by definition a set of positions that carries enough information to fully determine the message corresponding to a codeword.
Indeed, given the value of a codeword on an information set, one can reconstruct the entire (non-noisy) codeword by simply applying an encoder systematic w.r.t. this set;
it is then easy to invert the encoding to go back to the original message.
Thus, what the above does is (randomly) trying to find an information set over which the error vector is all-zero.

\smallskip

A variant of the above first algorithm due to Lee and Brickell~\cite{LeeBrickell} consists, for each information set, in checking the weight of all linear combinations of rows of $\bm{G}'$ of weight less than a small value $p$ (typically 2 or 3). This somehow amortizes
the cost of computing $\bm{G}'$ by considering more codewords for each matrix, as now the algorithm returns on a given information set if
a codeword's weight splits as $(i,w-i), 1 \leq i \leq p$ over itself and its complement.
Also, note that for binary codes, computing all of these can be done particularly efficiently by using Gray codes.

\smallskip

Another variant due to Leon aims to reduce the practical cost of checking the weight of a codeword, and is thus mostly useful for long codes. The idea is simply to first check
if a codeword generated from the above procedure has a small weight on a few positions (i.e. to first consider a short \emph{punctured} code), and only to look at the entire
codeword in that case. For instance, if one requires the punctured codeword to have weight zero on its redundancy set of size $l$, one is in effect searching for codewords
whose weight splits as $(i,0,w-i), 1 \leq i \leq p$ over the information set, and the non-punctured (resp. punctured) redundancy positions.

While this approach looks at fewer candidates per information set as the Lee-Brickell algorithm, this is hoped to be counter-balanced by more efficient implementations.

\medskip

Another algorithm due to Stern still follows the overall same approach, but improves the time complexity at the cost of some memory~\cite{Stern}. The key idea is to split the search
space into two lists and to exploit collisions to obtain a quadratic speed-up at some stage of the search. Starting from the initial algorithm, one splits the information set
into two subsets $I_1$ and $I_2$, and forms the lists $\Lambda_1$ and $\Lambda_2$ of codewords of weight less than $p$ on each subset respectively. Then one only fully checks the weight of
codewords formed by the sum of elements of $\Lambda_1$ and $\Lambda_2$ that are identical over $l$ prescribed positions $Z$ of the redundancy set. One is then searching for codewords
whose weight splits as $(i,j,0,w-i-j), 1 \leq i,j \leq p$ over $I_1$, $I_2$, $Z$ and the remainder.

It is essential to notice that for a given information set, checking for each of the $\#\Lambda_1\#\Lambda_2$ candidate codewords if it is of the above form indeed takes a cost
linear in $\#\Lambda_{1,2}$ (by using an appropriate data structure).

Finally, one may remark that this algorithm takes more input parameters than the previous ones. This, together with the fact that it is not memory-less may make it harder to
determine what parameter choice is best suited to a given code.

\medskip

An important observation made by Canteaut and Chabaud~\cite{CanteautChabaud} is that the most expensive step in the above algorithms is the computation of the systematic encoder for a given
information set. They then suggest that instead of selecting a new independent information set at every iteration, one may ``update'' the current set by randomly replacing one
of its columns by one column of the redundancy set, which is much more efficient. Furthermore, one can easily be convinced that after a few iterations, the obtained information
set will be essentially independent from the starting one, hence there is no risk that one gets stuck in a small subset of the search space considered by the other algorithms.

We will conclude by describing how to efficiently update an information set. Let $\bm{G} = \begin{pmatrix}\bm{I} & \bm{A}\end{pmatrix}$ be a systematic generator matrix;
our objective is to compute $\bm{G}' = \begin{pmatrix}\bm{I} & \bm{A}'\end{pmatrix}$ which is a generator matrix for the same code and equal to the
reduced row-echelon form of a matrix obtained from $\bm{G}$ by swapping one column $\bm{I}_{\cdot,i}$ of the identity with one column
$\bm{A}_{\cdot,j}$ of the redundancy matrix. First notice that this latter
process only results in a systematic matrix if $\bm{A}_{\cdot,j}$ is linearly independent from $\bm{I}\backslash\bm{I}_{\cdot,i}$, which is equivalent to
requiring that $\bm{A}_{i,j} = 1 \neq 0$. Second, the matrix $\bm{A}'$ is simply obtained from $\bm{A}$ by adding the row $\bm{A}_{i}$
to every row $\bm{A}_{i'}$ where $\bm{A}_{i',j} = 1$. Indeed, this corresponds to the ``reduction'' step one needs to perform after swapping the above two columns.

\section{Learning Parity with Noise cryptosystems}

In this section, we introduce the \emph{Learning Parity with Noise} (LPN) problem, and its application to the design of cryptosystems.
The LPN problem is rather attractive because of its very concise description. Let $\bm{s} \in \ftwo^k$, $\bm{a} \randraw \ftwo^k$,
$e \leftarrow \Ber_\eta$, where $\Ber_\eta$ is the Bernoulli distribution of parameter $\eta$; that is,
$\Pr[e = 1] = \eta$. Then the LPN problem is to guess the value of the
scalar product $\bm{s} \cdot \bm{a}$ when given $(\bm{a},\bm{s} \cdot \bm{a} + e)$. An algorithm is then said to solve
this problem with advantage $\varepsilon$ if it answers correctly with probability $p$ and $2|p - 1/2| = \varepsilon$.

It is clear that if $\eta = 1/2$, the distribution of $\bm{a}\cdot\bm{s} + e$ is independent of $\bm{a}$ and $\bm{s}$, and no algorithm
can succeed with a non-zero advantage. Similarly, without prior knowledge about $\bm{s}$ and except when $\bm{a} = \bm{0}$, one cannot hope
to solve the problem given a \emph{single} query of the above form, even in the absence of noise (i.e. even when $\eta = 0$). It is then natural to extend the problem to arbitrarily-many queries $q$, where
the unknown (secret) vector $\bm{s}$ is kept constant. One may then reformulate the problem as letting $\bm{A} \randraw \ftwo^{k\times q}$,
$\bm{e} \leftarrow \Ber_{\eta,q}$ (that is, each bit of $\bm{e} \in \ftwo^q$ has independent probability $\eta$ to be equal to one), and asking to distinguish
$(\bm{A},\bm{s}\bm{A} + \bm{e})$ from $(\bm{A}, \bm{u})$ where $\bm{u} \randraw \ftwo^q$. An algorithm is then said to $(t,q)$ solve LPN$_{k,\eta}$ with advantage $\varepsilon$
if it makes $q$ queries and has running time $t$.

It is now again clear that if $\eta = 0$, the problem is trivially solvable as long as $\rank(\bm{A}) = k$, as it is then enough to identify $k$ linearly-independent columns of $\bm{A}$ to recover
$\bm{s}$ (which then allows to predict all the other queries with advantage 1). In the more meaningful case where $\eta > 0$, recovering $\bm{s}$ becomes equivalent to decoding a noisy codeword for
some random code of length $q$ (which is a parameter that may be chosen by the solving algorithm).
We will discuss this matter in more details in the next section, and focus for now on some LPN-based cryptosystems whose security depends on the computational hardness of this problem.

\bigskip

We first describe LPN-C, which is a family of symmetric encryption schemes defined by Gilbert et al.~\cite{LPNC}. While the confidentiality of an LPN-C instance reduces to the hardness of a corresponding
LPN problem, the scheme is inherently malleable and must thus be used in conjunction with a MAC.

An LPN-C instance is parameterized by a random code length $k$, a noise level $\eta$, a message length $r$, and the parameters of an $[m,r,d]$ binary code $\code$, assumed to be efficiently decodable up to $w$ errors and s.t. $\Pr[\wt(\bm{e}) > w : \bm{e} \leftarrow \Ber_{\eta,m}]$
is small.
The scheme works as follows. The sender and the receiver first share a secret random matrix $\bm{M} \in \ftwo^{k\times m}$. Then, to encrypt an $r$-bit message $\bm{x}$, the sender
draws a vector $\bm{a} \randraw \ftwo^k$ and $\bm{e} \leftarrow \Ber_{\eta,m}$, computes $\bm{y} = \code(\bm{x}) + \bm{a}\bm{M} + \bm{e}$, and sends $(\bm{a},\bm{y})$ to the receiver.
To decrypt, the receiver computes $\hat{\bm{x}} = \bm{y} + \bm{a}\bm{M} = \bm{x} + \bm{e}$, and uses the decoder of $\code$ to recover $\bm{x}$.
The designers of LPN-C proposed some parameters for secure instantiations of LPN-C, but without specifying which code $\code$ to choose. An example is to take $k = 768$, $m = 160$, $\eta = 1/20$, $r = 75$, $d = 25$.

An informal way to argue about the security of this scheme is that if one uses parameters for which the LPN problem is hard to solve, then the (encoded) message $\code(\bm{x})$ is whitened
by a pseudo-random mask $\bm{a}\bm{M} + \bm{e}$ which is hard to distinguish from random, and is thus encrypted by a secure ``stream cipher''.\footnote{There is a slight difference between this
setting and the actual definition of LPN that we have used: here the \emph{matrix} is secret and the vector $\bm{a}$ is public. Yet, this simply corresponds to $m$ ``single'' queries for $m$
independent secrets batched together into $\bm{M}$.}

\medskip

We now turn to an LPN-based public-key cryptosystem due to Alekhnovich~\cite{alekh03}. This is a highly impractical design, as it only encrypts a single bit and the decryption of ``1'' fails with
probability 1/2. It is however of theoretical interest, and is rather simple to describe.

This scheme is parameterized by an integer $n$, from which one derives $m = 2n$, $k = n^{1/2-\epsilon}$, $\eta = k/n$. The public key is a matrix $\bm{A}' \in \ftwo^{n+1\times m}$, generated
as $\begin{pmatrix}\bm{A}\\ \bm{\hat{c}}\\ \end{pmatrix}$ where $\bm{A} \randraw \ftwo^{n\times m}$,  $\bm{\hat{c}} = \bm{x}\bm{A} + \bm{e}$,   $\bm{x} \randraw \ftwo^n$, $\bm{e} \leftarrow \Ber_{\eta,m}$.
The private key is the vector $\bm{e}$.
In other words, one defines a random $[2n,n]$ code with generating matrix $\bm{A}$ and augments it with a low-weight codeword $\bm{e}$ to the code $\code$ generated by $\bm{A}'$. One can indeed check
that $\bm{e}$ is in the span of $\bm{A}'$, as it is equal to $\begin{pmatrix} \bm{x} & 1 \end{pmatrix}\bm{A}'$.\footnote{One may also remark that for the chosen parameters, $\bm{e} \notin \vspan(\bm{A})$
with high probability. In the unlikely event where this would be the case, one can simply choose another vector and try again.}
This augmentation is however done in a ``hidden'' way, as recovering $\bm{e}$
from $\bm{A}'$ is a (hard) decoding problem for the code defined by $\bm{A}$.

To encrypt one bit for the public key $\bm{A}'$, the sender proceeds as follows. To encrypt the bit 1, it sends a vector $\bm{\alpha_1} \randraw \ftwo^m$. To encrypt the bit 0,
it computes and sends $\bm{\alpha_0} = \bm{d} + \bm{e}'$, where $\bm{e}' \leftarrow \Ber_{\eta,m}$ and $\bm{d}$ is a uniformly random element of $\code^\bot$.
The receiver computes the decryption of $\bm{\alpha}$ as $\bm{\alpha} \cdot \bm{e}$.

It is quite immediate to see that upon receiving $\bm{\alpha_1}$, decryption will fail with probability 1/2. It is a bit less obvious that the decryption
of $\bm{\alpha_0}$ is more successful. In that case, the receiver computes $\bm{\alpha_0} \cdot \bm{e} = \bm{d}\cdot\bm{e} + \bm{e}\cdot\bm{e}'$. Because
$\bm{d} \in \code^\bot$, $\bm{e} \in \code$, this simplifies to $\bm{e}\cdot\bm{e}'$. Finally, the probability that $\bm{e}$ and $\bm{e}'$ have a non-disjoint
support is $\approx (1-\eta)^k$, which is negligible.

We already have informally argued that computing the secret key $\bm{e}$ from the public key $\bm{A}'$ reduces to a hard decoding problem. We may now also
remark that distinguishing $\bm{\alpha_0}$ from $\bm{\alpha_1}$ reduces to an LPN-like problem for a generating matrix of $\code^\bot$. Alekhnovich then
showed that the latter is computationally indistinguishable from a random matrix, which allows to conclude the reduction to LPN.

\medskip

We conclude this overview by describing an LPN-based symmetric authentication protocol named Lapin~\cite{lapin}. Strictly speaking, Lapin is based on the Ring-LPN variant
of the problem, whose aim is to decrease the communication complexity. We will first describe it in the LPN framework, and will address this difference next.

A challenger and a verifier share two secret vectors $\bm{s}$, $\bm{s}' \in \ftwo^n$. To authenticate the challenger, the verifiers draws $\bm{C} \randraw \ftwo^{n\times n}$
and send it to the former. The challenger then draws $\bm{R} \randraw \gl(n, \ftwo)$, $\bm{e} \leftarrow \Ber_{\eta,n}$, and sends $(\bm{R}, (\bm{s}\bm{C} + \bm{s}')\bm{R} + \bm{e})$.
The verifier then recovers $\bm{e}$ and validates the challenge if it is of weight less than $\mu \eta n$ for some small acceptance threshold $\mu$.

The Ring-LPN variant of the protocol works similarly, but works over rings of the form $\ftwo[X]/\langle f \rangle$, for some polynomial $f$. The challenge matrix $\bm{C}$
is replaced by a ring element $\proj(c)$, $c \in \ftwo^\lambda$, where $\proj$ is a mapping verifying some conditions; the matrix $\bm{R}$ is replaced by an invertible
element of the ring $r$; the secrets are now also ring elements $s$ and $s'$, and so is the error $e$ (still drawn from a Bernoulli distribution, when seen as a vector).
The message sent by the challenger is then simply $(r, (s\proj(c) + s')r + e)$. The advantage of this variant over the matrix version of the protocol is that the ring elements
(esp. $r$) have a much more compact representation of size $\approx n$ than $n\times n$ random matrices, which essentially decreases the communication cost by a factor $n$.
However, the security now depends on the hardness of decoding codes posessing some structure, which may allow for more efficient algorithms.

Finally, a (still informal) way to argue about the security of this protocol is to notice that the challenger's answer can be written (in the original LPN case) as $\bm{s}\bm{CR} + \bm{s}'\bm{R} + \bm{r}$.
In other words, one is masking the string challenge-and-secret-dependent string $\bm{sCR}$ by an LPN query $\bm{s}'\bm{R} + \bm{e}$, which is by assumption computationally indistinguishable from random. 

\section{LPN solving algorithms}

It is quite clear that solving an LPN instance can be done by using generic decoding algorithms. Indeed, one may simply try to solve the \emph{search} variant of the LPN problem, which on $\bm{sA} + \bm{e}$ tries to
recover $\bm{s}$, i.e. tries to decode a noisy codeword of the code generated by $\bm{A}$. As solving the search problem allows to solve the distinguishing problem as well, the hardness of LPN
is not more than the one of decoding.

One subtlety that we already mentioned is that an LPN-solving algorithm is free to choose the number of oracle queries it wants to use, i.e. the \emph{length} of the code it wishes to decode. Surely this has
to be sufficiently large to even make a successful decoding possible (for instance one may try to ensure that there is at least one error-free information set w.h.p.), but there is no similar constraint
on the maximum number of queries.

We will now describe an algorithm that specifically exploits the ability to make many queries in an LPN problem to decrease the time complexity
for solving the (search) problem (at the cost of a potentially huge increase in memory and query complexity). This algorithm was first described in this context by Blum, Kalai and Wasserman~\cite{BKW},
and is traditionally called \emph{BKW}; we heavily base our presentation on the variant described by Bernstein and Lange~\cite{bunny}.

\smallskip

We first describe a simple variant of BKW. An intuition behind the algorithm is that given sufficiently many LPN queries $\bm{s}\cdot\bm{a} + e$ with a \emph{fixed}, vector $\bm{a}$,
one can efficiently recover one bit of $\bm{s}$ (viz. $\bm{s}\cdot\bm{a}$) using a majority vote among the samples. This requires $\approx c^{-2}$ samples, where $c = 1 - 2\eta$ ($0 < \eta < 1/2$)
is the correlation 
of the noise. Repeating this process for $k$ linearly independent masks $\bm{a}$ leads to a full recovery of $\bm{s}$.

Of course, as the masks $\bm{a}$ of LPN queries are uniformly random, collecting enough samples for a single one is equivalent to observing a $c^{-2}$-multi-collision, which
requires $2^{k\cdot(c^{-2}-1)/c^{-2}} \approx 2^k$ samples, so this does not really improve on the exhaustive search of the secret. The idea behind BKW is then to artificially
create samples of the above form by combining many random ones; this may provide enough samples to apply majority decoding, but each sample is now ``noisier'', and one must then find
a proper tradeoff.

More precisely, BKW proceeds as follows. To solve an instance of LPN with noise level $\eta$ (i.e. noise correlation $c = |1-2\eta|$) and dimension $k$ with $q$ queries making a pool $P_0$, we start by fixing
a block-size parameter $b$. Then one creates a table $T$ of size $2^b$ and an updated pool $P_1$, both initially empty. Next, for each sample  $x = (\bm{a}, v = \bm{a}\cdot\bm{s} +e) \in P$, do the following:
\begin{enumerate}
	\item Call $i$ the integer value corresponding to the last $b$ bits of $\bm{a}$. If $T[i] = \emptyset$, update it as $T[i] \mapsfrom (\bm{a}, v)$.
	\item Else, retrieve $(\bm{a}',v')$ from $T[i]$ and update $x$ as $(\bm{a},v) \mapsfrom (\bm{a}+\bm{a}',v+v')$ and store it in $P_1$.
\end{enumerate}
At the end of this process, and provided that $q \gg 2^b$, there are $q-2^b$ samples in $P_1$ which all have their masks $\bm{a}$ equal to zero on their last $b$ bits. However, the corresponding dot
products indeed became noisier, the correlation $c$ having been squared.\footnote{One can check that the probability of error is $2\eta(1-\eta) = 2(\eta-\eta^2)$, giving a correlation
$1-4(\eta-\eta^2) = (1-2\eta)^2$.} One may now clear the table $T$ of all its entries and start this process again, obtaining a pool $P_2$ of $q - 2^{b+1}$ samples with masks whose
$2b$ last bits are zero and noise correlation $c^4$, etc., up to a pool $P_t$ of $q - t2^b$ samples with masks with $tb$ zeroes and noise correlation $c^{2^t}$.

The original algorithm chooses $t$ and $b$ s.t. $tb = k-1$, that is the last pool is made of samples with masks $\bm{a}$ of the form $(0,\ldots,0)$, which are useless, and $(1,0,\ldots,0)$,
whose corresponding $v$ values can be used to find the first bit of $\bm{s}$ by a majority vote. This latter step will succeed w.h.p. if $\#P_t \approx c^{-2^{t+1}}$. Finally, the remaining
bits of $\bm{s}$ are iteratively retrieved using the same process.

\medskip

We now introduce a first optimization to the above algorithm, due to Levieil and Fouque~\cite{LF}. Let $l$ be a second ``block size'' parameter, corresponding to an exhaustive search step.
The goal is to guess $\bm{s}$ by blocks of $l$ bits instead of blocks of 1; the first iteration of BKW is then stopped at $tb = k -l$. At that point, one has $N := q - t2^b$ noisy
dot products with masks $\bm{a}_i$ whose $l$ first bits only may be non-zero.\footnote{Note that as $N \gg l$, \emph{non-noisy} approximations would indeed allow to uniquely recover the first $l$ bits of $\bm{s}$.}
Let $\bm{y} \in \ftwo^k$ be a vector whose $k-l$ last bits are zero; the idea is to notice that if $\bm{y}$ and $\bm{s}$ are equal on their first $l$ bits, then $\bm{a}_i \cdot \bm{y} = \bm{a}_i \cdot \bm{s}$.
Consequently, for every sample $(\bm{a}_i, v_i)$, one has $\bm{a}_i \cdot \bm{y} + v_i = e_i$ (where $e_i$ is an aggregated error of some correlation $c'$ coming from the pool creation) which is then
one with non-uniform probability $(1-c')/2$. On the other hand,
if $\bm{y}$ and $\bm{s}$ disagree on their last bits, $\bm{a}_i \cdot \bm{y} + v_i = \bm{a_i} \cdot (\bm{y} + \bm{s}) + e_i$ for a non-zero term $\bm{y}+\bm{s}$, and from the uniformity of $\bm{a}_i$ on its first $l$ bits,
this expression is one with probability $1/2$ exactly. If $N \approx c'^{-2}$, it is thus possible to distinguish the value of $\bm{y}$ that matches $\bm{s}$ to recover its last $l$ bits.

%The full process for one block of size $l$ is as follows:
%\begin{enumerate}
%	\item For all $2^l$ candidates $\bm{y_i}$ for the first $l$ bits of $\bm{s}$, compute $\walsh(\bm{y}_i) = \sum_{j = 0}^N (-1)^{\bm{a}_j\cdot\bm{y}_i+v_j}$ (where the sum is over $\mathbb{Z}$).
%	\item Return the $\bm{y}_i$ for which $|\walsh(\bm{y}_i)|$ is largest.
%\end{enumerate}
%A key algorithmic observation is that if $N$ equals $2^m$ a power of two, the above corresponds to the computation of the Hadamard-Walsh spectrum of the Boolean function $i \mapsto v_i$, which can be done
%efficiently thanks to a fast-fourier-like process in time $m2^m$, using the recursion:
%\[
%	\walsh(\bm{y}) = \sum_{j = 0}^{2^m} (-1)^{\bm{a}_j\cdot\bm{y}+v_j}% = \walsh(\bm{
%\]

The full process for one block of size $l$ is as follows:
\begin{enumerate}
	\item For all $2^l$ candidates $\bm{y_i}$ for the first $l$ bits of $\bm{s}$, compute $\hat{c}_{\bm{y}_i} = \sum_{j = 0}^N (-1)^{\bm{a}_j\cdot\bm{y}_i+v_j}$ (where the sum is over $\mathbb{Z}$).
	\item Return the $\bm{y}_i$ for which $|\hat{c}_{\bm{y}_i}|$ is largest.
\end{enumerate}
A key algorithmic observation is that, the above is similar to the computation of the Hadamard-Walsh spectrum of the Boolean function $i \mapsto v_i$, for which fast algorithms exist. One can indeed
remark that for $\bm{y}$ and $\bm{y}'$ that differ only on a single bit, on position $l$ (w.l.o.g.), if we write $\bm{y}''$ the common part of the two vectors of length $l-1$ and
$\bm{a}_j^1$ (resp. $\bm{a}_j^0$) the masks whose $l^\text{th}$ bit is 1 (resp. 0); then we have the following; let
\begin{align*}
A := \sum_{\bm{a}_j^1}(-1)^{\bm{a}_j\cdot\bm{y}+v_j}, \qquad&
\quad B := \sum_{\bm{a}_j^0}(-1)^{\bm{a}_j\cdot\bm{y}''+v_j}, &
-A = \sum_{\bm{a}_j^1}(-1)^{\bm{a}_j\cdot\bm{y}'+v_j},
\end{align*}
and	$\hat{c}_{\bm{y}} = A + B$, $\hat{c}_{\bm{y}'} = -A + B$, and this equality can be applied $l$ times recursively.

\medskip

The above idea can be further optimized by first applying a transformation on the LPN samples in order to reduce the problem to finding an equivalent \emph{low weight} secret. This idea is due
to Kirchner~\cite{paul_lpn} and works as follows. Denote $\bm{A}$ the full matrix of the masks of $q$ queries to an LPN oracle of dimension $k$. Let $\bm{A}_1$ be an information set of $k$
linearly-independent columns of $\bm{A}$, and let $\bm{A}_2$ denote $k$ columns of $\bm{A}$ not in $\bm{A}_1$. The key observation is that the sum $(\bm{s}\bm{A}_1 + \bm{e}_1)\bm{A}_1^{-1}\bm{A}_2
+ \bm{s}\bm{A}_2 + \bm{e}_2$ simplifies to $\bm{e}_1\bm{A}_1^{-1} + \bm{e}_2$. In other words, one can transform the $2k$ queries w.r.t. masks $\bm{A}_1$ and $\bm{A}_2$ for the secret $\bm{s}$
into $k$ queries for the ``secret'' $\bm{e}_1$, whose expected Hamming weight is given by the noise level $\eta$, and is then strictly less than $k/2$. The same transformation can
be applied many times by changing $\bm{A}_2$, and one can then run a BKW algorithm to retrieve $\bm{e}_1$ (which obviously immediately leads to $\bm{s}$). There is no particular advantage
in using this transformation when applying the original BKW algorithm, but when one guesses the secret by block, it becomes enough to guess secrets of low weight, which reduces
the search space. Finally, note that the joint computation of the many correlations $\hat{c}_{\bm{y}}$ can still be done efficiently for those sparse secrets.

\medskip

We conclude this section by mentioning that even prior the publication of the BKW algorithm, Bleichenbacher described a similar algorithm to exploit biased DSA signatures~\cite{bleichenbacher, ac14bleich}.
Recall that in a DSA (or Schnorr) signature, the signer provides a pair $(c,r+cx)$, where $x$ is a secret exponent, $c$ is random and depends on the message to be signed, and $r$ is
a random mask. If $r$ is uniform, the secret $cx$ is blinded by a one-time-pad and nothing can be learned. But if $r$ is biased, then one obtains a problem of noisy decoding similar
to LPN. Bleichenbacher's algorithm to retrieve $x$ from many signatures consists in combining samples to zero some of their bits, and to apply a fast Fourier transform to recover,
say, 40 bits of the secret. The original bias exploited by Bleichenbacher was a small modulo bias: instead of taking $r$ uniformly in $\mathcal{S}$, $\#\mathcal{S} \approx 2^{160}$,
it was uniform over $[0,\ldots,2^{160}-1]$ and then reduced modulo $\#\mathcal{S}$.
Finally, we remark that similarly to information-set decoding style algorithms, one can also recover a DSA secret using fewer sample signatures with biased masks by finding short vectors in
a Euclidean lattice~\cite{howgrave_grahm_Smart_dsa}.

\section{The Goldreich-Levin theorem}

In this section, we present the application of list decoding to a proof of existence of \emph{hard-core} predicates for one-way functions, due to Goldreich and Levin~\cite{gl}. Informally,
the objective is to show that if $\fun : \zo^n \rightarrow \zo^m$ is a one-way function, in the sense that it is hard to find a preimage $x$ given $\fun(x)$, then it is also hard to predict
(with probability significantly away from $1/2$)
the value $\bm{a}\cdot\bm{x}$, for any $\bm{a} \in \ftwo^n$ (where $\bm{x} \in \ftwo^n$ is the canonical embedding of $x \in \zo^n$). A possible proof is to show that if one is given
a prediction oracle for (several) $\bm{a}\cdot\bm{x}$ with correlation $c$, then one can reconstruct $x$ w.h.p. with time and memory complexity $c^{-2}$. Thus, if ``it costs at least $T$
to invert $\fun$'', one has that it is impossible to predict the value of any of the above predicates with correlation ($\approx$ advantage) better than $1/\sqrt{T}$.

One first subtlelty that deserves to be mentioned is that because of the nature of the result we want to prove, the predicate oracle that we will use to invert $\fun$ can
only be called \emph{once} for a given mask $\bm{a}$: indeed, it makes no sense to define different predictions $\bm{a}\cdot\bm{x}$ several times as $\bm{x}$ itself is fixed. On the other
hand, the mask $\bm{a}$ can be chosen freely. This is to be contrasted with, say, an LPN setting, where one is given $\bm{a}\cdot\bm{s}$ for random masks $\bm{a}$, that may
still be potentially equal, in case of unlikely collisions. Yet in either case the effect is the same: it is not (by definition or computationally) feasible to recover $\bm{x}$ by recovering enough
biased predictions or samples with the same $n$-linearly independent masks. Finally, an accurate modelisation of the oracle's power in terms of codes is to say that if $\bm{A} \in \ftwo^{n\times q}$
has $q$ pairwise distinct columns, querying a prediction oracle with $q$ masks $\bm{A}$ and advantage $\varepsilon$ (meaning that a prediction is correct with probability
$p > 1/2$; $2p - 1 = \varepsilon$) is equivalent to obtaining a noisy codeword $\bm{x}\bm{A} + \bm{e}$ with $\bm{e} \leftarrow \Ber_\eta,q$, $\eta = 1/2 - \varepsilon/2$. 

A second essential remark is that $\fun$ and $\fun(x)$ are both known to the adversary. Thus, as soon as one knows a ``small'' list $L$ that contains $x$ w.h.p., one can recover
$x$ uniquely by mapping $\fun$ to $L$ and comparing the result to $\fun(x)$ (here we assume that $L$ does not contain collisions for $\fun$, which is true w.h.p. if $L$ is small).

Putting the two remarks together, what we need is an efficient \emph{list-decoding} algorithm for a code generated by a matrix $\bm{A}$ with pairwise-distinct columns. In the remainder,
we will give exactly such an algorithm for a punctured \emph{Hadamard code} (that is a first-order Reed-Muller code where messages have no constant term).


% objective, constraints

% list-decoding of punctured RM/Hadamard

\bibliographystyle{alpha}
\bibliography{tehbib}

\end{document}
